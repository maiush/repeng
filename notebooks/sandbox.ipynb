{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-22 13:40:57,084] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x71a610e5fe20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from repeng import ControlModel, ControlVector, DatasetEntry\n",
    "from personality.utils import load_model_and_tokenizer\n",
    "from personality.constants import MODEL_PATH, DATA_PATH, CONSTITUTION_PATH\n",
    "\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama-3.1-8b-it\"\n",
    "constitution = \"sarcasm\"\n",
    "steering_prompt_type = \"long\"\n",
    "adversarial = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565b97576174444e9a6533b4a0e2d64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LOAD CONTROL MODEL AND TOKENIZER ===\n",
    "model, tokenizer, nlayers = load_model_and_tokenizer(\n",
    "    f\"{MODEL_PATH}/{model_name}\",\n",
    "    get_n_layers=True\n",
    ")\n",
    "start = int(0.125 * nlayers)\n",
    "end = int(0.875 * nlayers)\n",
    "model = ControlModel(model, range(start, end))\n",
    "\n",
    "# === LOAD DATA ===\n",
    "train = pd.read_json(f\"{DATA_PATH}/wildchat/train.jsonl\", orient=\"records\", lines=True)\n",
    "test = pd.read_json(f\"{DATA_PATH}/wildchat/test.jsonl\", orient=\"records\", lines=True)\n",
    "train_messages = train[\"messages\"].tolist()\n",
    "test_messages = test[\"messages\"].tolist()\n",
    "# === ADD ADVERSARIAL PROMPT IF NEEDED === \n",
    "if adversarial:\n",
    "    for idx in range(len(train_messages)):\n",
    "        train_messages[idx][-1][\"content\"] += \"\\n\\n(Respond in the neutral yet cheerful tone of an AI Assistant.)\"\n",
    "    for idx in range(len(test_messages)):\n",
    "        test_messages[idx][-1][\"content\"] += \"\\n\\n(Respond in the neutral yet cheerful tone of an AI Assistant.)\"\n",
    "# apply chat template\n",
    "train_prompts = tokenizer.apply_chat_template(train_messages, tokenize=False, add_generation_prompt=True)\n",
    "test_prompts = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:30<00:00,  1.61s/it]\n",
      "100%|██████████| 31/31 [00:22<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# === PREP STEERING VECTORS ===\n",
    "def train_steering_vector(\n",
    "        model: ControlModel,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        steering_persona: str,\n",
    "        default_persona: str = \"anything\"\n",
    ") -> None:\n",
    "    with open(\"/workspace/repeng/notebooks/data/all_truncated_outputs.json\") as f:\n",
    "        output_suffixes = json.load(f)\n",
    "    # reset any existing steering vectors\n",
    "    model.reset()\n",
    "    steering_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"Please talk about {steering_persona}.\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    default_prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"Please talk about {default_persona}.\"}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    dataset = []\n",
    "    for suffix in output_suffixes:\n",
    "        dataset.append(\n",
    "            DatasetEntry(\n",
    "                positive=steering_prompt + suffix,\n",
    "                negative=default_prompt + suffix,\n",
    "            )\n",
    "        )\n",
    "    return ControlVector.train(\n",
    "        model, tokenizer, dataset, method=\"pca_center\", batch_size=64\n",
    "    )\n",
    "cache = {}\n",
    "def vec(steer_string: str):\n",
    "    if steer_string not in cache:\n",
    "        cache[steer_string] = train_steering_vector(model, tokenizer, steer_string)\n",
    "    return cache[steer_string]\n",
    "assert steering_prompt_type in [\"long\", \"short\"]\n",
    "if steering_prompt_type == \"long\":\n",
    "    cons = pd.read_json(f\"{CONSTITUTION_PATH}/hand-written/{constitution}.txt\")\n",
    "    traits = \"\\n\".join([f\"{i+1}: {trait}\" for i, trait in enumerate(cons[\"trait\"])])\n",
    "    steering_prompt = f\"responding as an AI assistant whose primary character-traits are determined by the following information:\\n\\n{traits}\"\n",
    "elif steering_prompt_type == \"short\":\n",
    "    steering_prompt = f\"responding with {constitution} all the time\"\n",
    "v = vec(steering_prompt) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training set:   2%|▏         | 16/1000 [05:08<5:39:38, 20.71s/it]"
     ]
    }
   ],
   "source": [
    "# === GENERATE ===\n",
    "settings = {\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"temperature\": 1.0,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"max_new_tokens\": 512\n",
    "}\n",
    "model.reset()\n",
    "model.set_control(v)\n",
    "# train\n",
    "train_outputs = []\n",
    "for prompt in tqdm(train_prompts, desc=\"training set\"):\n",
    "    tks = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    prompt_len = len(tks.input_ids.squeeze(0))\n",
    "    with t.inference_mode():\n",
    "        out = model.generate(**tks, **settings)\n",
    "        out_tks = out.squeeze(0)[prompt_len:]\n",
    "    generation = tokenizer.decode(out_tks).strip()\n",
    "    if tokenizer.eos_token in generation:\n",
    "        generation = generation[:generation.rindex(tokenizer.eos_token)]\n",
    "    train_outputs.append(generation)\n",
    "# test\n",
    "test_outputs = []\n",
    "for prompt in tqdm(test_prompts, desc=\"test set\"):\n",
    "    tks = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    prompt_len = len(tks.input_ids.squeeze(0))\n",
    "    with t.inference_mode():\n",
    "        out = model.generate(**tks, **settings)\n",
    "        out_tks = out.squeeze(0)[prompt_len:]\n",
    "    generation = tokenizer.decode(out_tks).strip()\n",
    "    if tokenizer.eos_token in generation:\n",
    "        generation = generation[:generation.rindex(tokenizer.eos_token)]\n",
    "    test_outputs.append(generation)\n",
    "\n",
    "# === SAVE ===\n",
    "results = pd.DataFrame()\n",
    "results[\"prompt\"] = train_messages + test_messages\n",
    "results[\"split\"] = [\"train\"]*len(train_prompts) + [\"test\"]*len(test_prompts)\n",
    "results[\"response\"] = train_outputs + test_outputs\n",
    "# return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
